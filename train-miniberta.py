# -*- coding: utf-8 -*-
"""BabyBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aYyr6dOkHzC1BsOYJwa8a4yVnVCm4W3g
"""

# !pip install transformers==4.3.3
# !pip install datasets

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import torch
import seaborn as sns
import transformers
import json
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaModel, RobertaTokenizer
import logging
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel
logging.basicConfig(level=logging.ERROR)
from tokenizers import ByteLevelBPETokenizer
from tokenizers.processors import BertProcessing

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'

from pathlib import Path
from typing import List, Dict, Any, Tuple
import yaml
import random

def load_sentences_from_file(file_path: Path,
                             include_punctuation: bool = True,
                             allow_discard: bool = False,
                             ) -> List[str]:
    """
    load sentences for language modeling from text file
    """

    print(f'Loading {file_path}', flush=True)

    res = []
    num_too_small = 0
    
    with open(file_path, 'r') as line_by_line_file:
    # with file_path.open('r') as line_by_line_file:

        for sentence in line_by_line_file.readlines():

            if not sentence:  # during probing, parsing logic above may produce empty sentences
                continue

            sentence = sentence.rstrip('\n')

            # check  length
            if sentence.count(' ') < 3 - 1 and allow_discard:
                num_too_small += 1
                continue

            if not include_punctuation:
                sentence = sentence.rstrip('.')
                sentence = sentence.rstrip('!')
                sentence = sentence.rstrip('?')

            res.append(sentence)

    if num_too_small:
        print(f'WARNING: Skipped {num_too_small:,} sentences which are shorter than {3}.')

    return res

from itertools import islice

def make_sequences(sentences: List[str],
                   num_sentences_per_input: int,
                   ) -> List[str]:

    gen = (bs for bs in sentences)

    # combine multiple sentences into 1 sequence
    res = []
    while True:
        sentences_in_sequence: List[str] = list(islice(gen, 0, num_sentences_per_input))
        if not sentences_in_sequence:
            break
        sequence = ' '.join(sentences_in_sequence)
        res.append(sequence)

    print(f'Num total sequences={len(res):,}', flush=True)
    return res

from datasets import Dataset, DatasetDict

from transformers.models.roberta import RobertaConfig, RobertaForMaskedLM, RobertaTokenizerFast
from transformers import DataCollatorForLanguageModeling, Trainer, set_seed, TrainingArguments

# from babyberta.io import load_sentences_from_file
# from babyberta.utils import make_sequences
# from babyberta import configs
# from babyberta.params import param2default, Params


def main():

    # params = Params.from_param2val(param2default)

    # get new rep
    rep = 0
    path_out = '/scratch/pbsjobs/axy327/' + str(rep)
    # while path_out.exists():
    #     rep += 1
    #     path_out = configs.Dirs.root / 'official_implementation' / str(rep)

    print(f'replication={rep}')

    training_args = TrainingArguments(
        report_to=None,
        output_dir=str(path_out),
        overwrite_output_dir=False,
        do_train=True,
        do_eval=False,
        do_predict=False,
        per_device_train_batch_size=16,
        learning_rate=1e-4,
        max_steps=160_000,
        warmup_steps=24_000,
        seed=rep,
        save_steps=40_000
    )

    logger = logging.getLogger(__name__)
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
    )
    logger.setLevel(logging.INFO)
    set_seed(rep)

    logger.info("Loading data")
    data_path = 'oscar.eo.txt'  # we use aonewsela for reference implementation
    sentences = load_sentences_from_file(data_path,
                                         include_punctuation=True,
                                         allow_discard=True)
    data_in_dict = {'text': make_sequences(sentences, 1)}
    datasets = DatasetDict({'train': Dataset.from_dict(data_in_dict)})
    print(datasets['train'])
    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at
    # https://huggingface.co/docs/datasets/loading_datasets.html.

    logger.info("Loading tokenizer")
    # tokenizer = RobertaTokenizerFast(vocab_file=None,
    #                                  merges_file=None,
    #                                  tokenizer_file=str('/scratch/aditya/babyberta.json'),)
    # 
    tokenizer = ByteLevelBPETokenizer()
    tokenizer.train(files="oscar.eo.txt", vocab_size=52_000, min_frequency=2, special_tokens=[
    "<s>",
    "<pad>",
    "</s>",
    "<unk>",
    "<mask>",
    ])
    tokenizer.save_model("Babyberta")
    tokenizer.save("byte-level-BPE.tokenizer.json")
    tokenizer = RobertaTokenizerFast(vocab_file=None,
                                     merges_file=None,
                                     tokenizer_file=str('byte-level-BPE.tokenizer.json')
    )
    
    # tokenizer = ByteLevelBPETokenizer(
    # "./Babyberta/vocab.json",
    # "./Babyberta/merges.txt",
    # )
    # tokenizer._tokenizer.post_processor = BertProcessing(
    #     ("</s>", tokenizer.token_to_id("</s>")),
    #     ("<s>", tokenizer.token_to_id("<s>")),
    # )
    # tokenizer.enable_truncation(max_length=512)
    logger.info("Initialising Roberta from scratch")
    config = RobertaConfig(vocab_size=52_000,
                           hidden_size=256,
                           num_hidden_layers=8,
                           num_attention_heads=8,
                           intermediate_size=1024,
                           initializer_range=0.02,
                           )
    model = RobertaForMaskedLM(config)
    # Preprocessing the datasets.
    # First we tokenize all the texts.
    text_column_name = "text"

    def tokenize_function(examples):
        # Remove empty lines
        examples["text"] = [line for line in examples["text"] if len(line) > 0 and not line.isspace()]
        return tokenizer(
            examples["text"],
            padding=True,
            truncation=True,
            max_length=128,
            # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it
            # receives the `special_tokens_mask`.
            return_special_tokens_mask=True,
        )
    logger.info("Tokenising data")
    tokenized_datasets = datasets.map(
        tokenize_function,
        batched=True,
        num_proc=4,
        remove_columns=[text_column_name],
        load_from_cache_file=True,
    )
    
    train_dataset = tokenized_datasets["train"]
    print(f'Length of train data={len(train_dataset)}')

    # Data collator will take care of randomly masking the tokens.
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,
                                                    mlm_probability=0.15)

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=None,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # Training
    trainer.train()
    trainer.save_model()  # Saves the tokenizer too


if __name__ == "__main__":
    main()

# !pip install shutil
# !unzip babyberta.zip

# #  model_results_folder_name = 'huggingface_official_reference'
# MAX_STEP = 260_000

# path_model_results = '/content/huggingface_official_reference'
# # if path_model_results.exists():
# #     shutil.rmtree(path_model_results)

# for path_model in ('content/official_implementation').glob(f'*/checkpoint-{MAX_STEP}'): # loading each checkpoint to see which one is best

#     # load model and tokenizer
#     roberta = RobertaForMaskedLM.from_pretrained(path_model)
#     roberta.cuda(0)
#     path_tokenizer_config = '/content/babyberta.json'
#     tokenizer = load_tokenizer(path_tokenizer_config, params.max_input_length)

#     step = MAX_STEP
#     rep = path_model.parent.name
#     save_path = path_model_results / str(rep) / 'saves'

#     # save basic model info
#     # if not (path_model_results / 'param2val.yaml').exists():
#     #     save_yaml_file(path_out=path_model_results / 'param2val.yaml',
#     #                     param2val={'framework': 'huggingface',
#     #                               'is_huggingface_recommended': True,
#     #                               'is_base': False,
#     #                               })

#     # for each probing task
#     for paradigm_path in configs.Dirs.probing_sentences.rglob('*.txt'):
#         do_probing(save_path,
#                     paradigm_path,
#                     roberta,
#                     step,
#                     params.include_punctuation,
#                     tokenizer=tokenizer,
#                     )